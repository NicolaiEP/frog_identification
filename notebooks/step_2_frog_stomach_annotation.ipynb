{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]  = \"python\"\n",
    "import mrcnn.visualize\n",
    "import mrcnn.config\n",
    "import mrcnn.model\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "import cv2\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = pathlib.Path(\"weights\")\n",
    "WEIGHTS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "WEIGHTS_CHECKPOINT_PATH = WEIGHTS_PATH.joinpath(\"checkpoints\")\n",
    "WEIGHTS_CHECKPOINT_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "COCO_WEIGHTS = WEIGHTS_PATH.joinpath(\"mask_rcnn_coco.h5\")\n",
    "#FROG_WEIGHTS = list(WEIGHTS_CHECKPOINT_PATH.glob(\"*.h5\"))[-1]\n",
    "#FROG_WEIGHTS = WEIGHTS_PATH.joinpath(\"mask_rcnn_frog_students.h5\")\n",
    "\n",
    "\n",
    "CLASS_NAME = \"frog_stomach\"\n",
    "CLASS_NAMES = [\"BG\", CLASS_NAME]\n",
    "FROG_IMAGES = \"./frog_photos\"\n",
    "FROG_DATASET = \"./pelophylax_lessonae\" \"\"#\"perara/pelophylax_lessonae\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from PIL import ImageDraw, Image\n",
    "import mrcnn.utils\n",
    "\n",
    "class FrogStomachDataset(mrcnn.utils.Dataset):\n",
    "\n",
    "    def load_dataset(self, images_dir: str, dataset_dir: str=None, is_train=True):\n",
    "        # Adds information (image ID, image path, and annotation file path) about each image in a dictionary.\n",
    "        self.add_class(\"dataset\", 1, CLASS_NAME)\n",
    "\n",
    "        images_dir_path = pathlib.Path(images_dir)\n",
    "        dataset_split = \"train\" if is_train else \"validation\"\n",
    "        ds = load_dataset(\n",
    "            FROG_DATASET,\n",
    "            name=\"default\",\n",
    "            splits=[dataset_split],\n",
    "            image_dir=images_dir_path,\n",
    "            dataset_dir=dataset_dir\n",
    "        )\n",
    "\n",
    "        for sample in ds[dataset_split]:\n",
    "            image_id = sample[\"image_id\"]\n",
    "            image_path = sample[\"image_path\"]\n",
    "            annotation = sample[\"segmentation\"]\n",
    "            width = sample[\"width\"]\n",
    "            height = sample[\"height\"]\n",
    "            category_id = sample[\"category_id\"]\n",
    "\n",
    "            self.add_image('dataset', image_id=image_id, path=image_path, annotation=annotation, width=width, height=height, category_id=category_id)\n",
    "\n",
    "    # Loads the binary masks for an image.\n",
    "    def load_mask(self, image_id):\n",
    "\n",
    "        image_info = self.image_info[image_id]\n",
    "        annotations = image_info['annotation']\n",
    "        width = image_info[\"width\"]\n",
    "        height = image_info[\"height\"]\n",
    "        category_id = image_info[\"category_id\"]\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "\n",
    "\n",
    "        mask = Image.new('1', (width, height))\n",
    "        mask_draw = ImageDraw.ImageDraw(mask, '1')\n",
    "        for segmentation in annotations:\n",
    "            try:\n",
    "                mask_draw.polygon(segmentation, fill=1)\n",
    "            except:\n",
    "                pass\n",
    "            bool_array = np.array(mask) > 0\n",
    "            instance_masks.append(bool_array)\n",
    "            class_ids.append(category_id)\n",
    "\n",
    "        mask = np.dstack(instance_masks)\n",
    "        class_ids = np.array(class_ids, dtype=np.int32)\n",
    "\n",
    "        return mask.astype(\"bool\"), asarray(class_ids, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrogStomachConfig(mrcnn.config.Config):\n",
    "    NAME = CLASS_NAME\n",
    "    GPU_COUNT = 1\n",
    "    NUM_CLASSES =  1 + 1\n",
    "\n",
    "\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "\n",
    "    IMAGES_PER_GPU = 16\n",
    "    STEPS_PER_EPOCH = 100\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "EPOCHS = 100\n",
    "class InferenceConfig(FrogStomachConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    USE_MINI_MASK = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nicol\\miniconda3\\envs\\dat304\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Could not find model directory under weights",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model_inference \u001b[38;5;241m=\u001b[39m mrcnn\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mMaskRCNN(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m                                        config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m      6\u001b[0m                                        model_dir\u001b[38;5;241m=\u001b[39mWEIGHTS_PATH)\n\u001b[1;32m---> 10\u001b[0m model_inference\u001b[38;5;241m.\u001b[39mload_weights(filepath\u001b[38;5;241m=\u001b[39m\u001b[43mmodel_inference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_last\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     11\u001b[0m                              by_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dat304\\lib\\site-packages\\mrcnn\\model.py:2093\u001b[0m, in \u001b[0;36mMaskRCNN.find_last\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dir_names:\n\u001b[0;32m   2092\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[1;32m-> 2093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   2094\u001b[0m         errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[0;32m   2095\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find model directory under \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir))\n\u001b[0;32m   2096\u001b[0m \u001b[38;5;66;03m# Pick last directory\u001b[39;00m\n\u001b[0;32m   2097\u001b[0m dir_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir, dir_names[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Could not find model directory under weights"
     ]
    }
   ],
   "source": [
    "config = InferenceConfig()\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    model_inference = mrcnn.model.MaskRCNN(mode=\"inference\",\n",
    "                                           config=config,\n",
    "                                           model_dir=WEIGHTS_PATH)\n",
    "\n",
    "\n",
    "\n",
    "    model_inference.load_weights(filepath=model_inference.find_last(),\n",
    "                                 by_name=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './pelophylax_lessonae'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m FrogStomachDataset()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFROG_IMAGES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFROG_DATASET\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m val_dataset\u001b[38;5;241m.\u001b[39mprepare()\n",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m, in \u001b[0;36mFrogStomachDataset.load_dataset\u001b[1;34m(self, images_dir, dataset_dir, is_train)\u001b[0m\n\u001b[0;32m     11\u001b[0m images_dir_path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(images_dir)\n\u001b[0;32m     12\u001b[0m dataset_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFROG_DATASET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdataset_split\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages_dir_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_dir\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m ds[dataset_split]:\n\u001b[0;32m     22\u001b[0m     image_id \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dat304\\lib\\site-packages\\datasets\\load.py:1719\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[0;32m   1716\u001b[0m ignore_verifications \u001b[38;5;241m=\u001b[39m ignore_verifications \u001b[38;5;129;01mor\u001b[39;00m save_infos\n\u001b[0;32m   1718\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   1720\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   1721\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   1722\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   1723\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   1724\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1725\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   1726\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   1727\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   1728\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1729\u001b[0m     use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[0;32m   1730\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1731\u001b[0m )\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dat304\\lib\\site-packages\\datasets\\load.py:1497\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[0;32m   1495\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   1496\u001b[0m     download_config\u001b[38;5;241m.\u001b[39muse_auth_token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m-> 1497\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m import_main_class(dataset_module\u001b[38;5;241m.\u001b[39mmodule_path)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dat304\\lib\\site-packages\\datasets\\load.py:1222\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   1217\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m   1218\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1219\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1220\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1221\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m-> 1222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1225\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1226\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dat304\\lib\\site-packages\\datasets\\load.py:1191\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1187\u001b[0m             msg\n\u001b[0;32m   1188\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. If the repo is private, make sure you are authenticated with `use_auth_token=True` after logging in with `huggingface-cli login`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1189\u001b[0m         )\n\u001b[0;32m   1190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m [sibling\u001b[38;5;241m.\u001b[39mrfilename \u001b[38;5;28;01mfor\u001b[39;00m sibling \u001b[38;5;129;01min\u001b[39;00m dataset_info\u001b[38;5;241m.\u001b[39msiblings]:\n\u001b[0;32m   1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[0;32m   1194\u001b[0m         path,\n\u001b[0;32m   1195\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1198\u001b[0m         dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[0;32m   1199\u001b[0m     )\u001b[38;5;241m.\u001b[39mget_module()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dat304\\lib\\site-packages\\datasets\\load.py:1163\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   1161\u001b[0m hf_api \u001b[38;5;241m=\u001b[39m HfApi(config\u001b[38;5;241m.\u001b[39mHF_ENDPOINT)\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m     dataset_info \u001b[38;5;241m=\u001b[39m \u001b[43mhf_api_dataset_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_api\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# noqa: catch any exception of hf_hub and consider that the dataset doesn't exist\u001b[39;00m\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1172\u001b[0m         e,\n\u001b[0;32m   1173\u001b[0m         (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1177\u001b[0m         ),\n\u001b[0;32m   1178\u001b[0m     ):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dat304\\lib\\site-packages\\datasets\\utils\\_hf_hub_fixes.py:152\u001b[0m, in \u001b[0;36mdataset_info\u001b[1;34m(hf_api, repo_id, revision, timeout, use_auth_token)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hf_api\u001b[38;5;241m.\u001b[39mdataset_info(\n\u001b[0;32m    146\u001b[0m         repo_id,\n\u001b[0;32m    147\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    148\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    149\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    150\u001b[0m     )\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# the `token` parameter is deprecated in huggingface_hub>=0.10.0\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dat304\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:92\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[0;32m     89\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[0;32m     90\u001b[0m ):\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 92\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dat304\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:142\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m     )\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have -- or .. in repo_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './pelophylax_lessonae'."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "val_dataset = FrogStomachDataset()\n",
    "val_dataset.load_dataset(\n",
    "    images_dir=str(FROG_IMAGES),\n",
    "    dataset_dir=str(FROG_DATASET),\n",
    "    is_train=False\n",
    ")\n",
    "val_dataset.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn import visualize\n",
    "import random\n",
    "\n",
    "dataset = val_dataset\n",
    "config = config\n",
    "\n",
    "image_id = random.choice(dataset.image_ids)\n",
    "\n",
    "image, image_meta, gt_class_id, gt_bbox, gt_mask = mrcnn.model.load_image_gt(dataset, config, image_id)\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "info = dataset.image_info[image_id]\n",
    "print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id,\n",
    "                                       dataset.image_reference(image_id)))\n",
    "\n",
    "# Run object detection\n",
    "results = model_inference.detect([image], verbose=1)\n",
    "\n",
    "# Display results\n",
    "r = results[0]\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], dataset.class_names, r['scores'], title=\"Predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
